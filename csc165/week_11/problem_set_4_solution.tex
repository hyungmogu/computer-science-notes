\documentclass[12pt]{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage{enumerate}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mdframed}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{xcolor}


\begin{document}
\title{Problem Set 4 Solution}
\author{Hyungmo Gu}
\maketitle

\section*{Question 1}
\begin{enumerate}[a.]
    \item

    \textbf{Statement:} $\forall f,g:\mathbb{N} \to \mathbb{R}^{+}$,
    $b \in \mathbb{R}^{+}$, $(g(n) \in \Theta(f(n))) \land (n_0 \in \mathbb{N},\:
    n \geq n_0 \Rightarrow f(n) \geq b \land g(n) \geq b) \land (b > 1) \Rightarrow
    \log_b(g(n)) \in \Theta(\log_b(f(n)))$

    \bigskip

    \textbf{Statement Expanded:} $\forall f,g:\mathbb{N} \to \mathbb{R}^{+}$,
    $b \in \mathbb{R}^{+}$, $\Bigl(\exists c_1,c_2,n_0 \in \mathbb{R}^{+},\:\forall n \in \mathbb{N},\:
    n \geq n_0 \Rightarrow c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n)\Bigr) \land \Bigl(\exists n_1 \in \mathbb{N},\:
    n \geq n_1 \Rightarrow f(n) \geq b \land g(n) \geq b \Bigr) \land \Bigl( b > 1 \Bigr) \Rightarrow
    \Bigl(\exists d_1,d_2,n_2 \in \mathbb{R}^{+},\:\forall n \in \mathbb{N},\: n \geq n_2
    \Rightarrow d_1 \cdot \log_b(g(n)) \leq \log_b(f(n)) \leq d_2 \cdot \log_b(g(n) \Bigr)$

    \bigskip

    \begin{proof}
        Let $f,g:\mathbb{N} \to \mathbb{R}^{+}$, and $b \in \mathbb{R}^{+}$. Assume
        $c_1 = 1$, $c_2 = b$, and $n_0 = 1$, and $n \in \mathbb{N}$ such that
        $n \geq n_0$ and $c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n)$. Assume $f(n)$
        and $g(n)$ are eventually $\geq b$. Assume $b > 1$. Let $d_1 = 1$, $d_2 = 2$,
        and $n_2 = n_0$. Assume $n \geq n_2$.

        \bigskip

        We need to show $d_1 \cdot \log_b g(n) \leq \log_b f(n) \leq d_2 \cdot \log_b g(n)$.

        \bigskip

        We will do so in two parts. One for $(d_1 \cdot \log_b g(n) \leq \log_b f(n))$ and
        the other for $(\log_b f(n) \leq d_2 \cdot \log_b g(n))$.

        \bigskip

        \textbf{Part 1 ($d_1 \cdot \log_b g(n) \leq \log_b f(n)$):}

        \bigskip

        The assumption tell us

        \begin{align}
            c_1 \cdot g(n) \leq f(n)
        \end{align}

        \bigskip

        Then, it follows from the fact $\forall x,y \in \mathbb{R}^{+}, x \geq y
        \Leftrightarrow \log x \geq \log y$

        \begin{align}
            \log (c_1 \cdot g(n)) &\leq \log (f(n))
        \end{align}

        \bigskip

        Then, using the fact $b > 1$, we can calculate

        \begin{align}
            \frac{\log (c_1 \cdot g(n))}{\log b} &\leq \frac{\log (f(n))}{\log b}\\
            \frac{\log (c_1) + \log (g(n))}{\log b} &\leq \frac{\log (f(n))}{\log b}
        \end{align}

        \bigskip

        Then,

        \begin{align}
            \frac{\log (g(n))}{\log b} &\leq \frac{\log (f(n))}{\log b}
        \end{align}

        by the fact $c_1 = 1$ and $\log c_1 = 0$.

        \bigskip

        Then, since $\frac{\log f(x)}{\log b} = \log_b f(x)$,

        \begin{align}
            \log_b (g(n)) &\leq \log_b (f(n))
        \end{align}

        \bigskip

        Then, because we know $d_1 = 1$, we can conclude

        \begin{align}
            \log_b (g(n)) &\leq d_1 \cdot \log_b (f(n))
        \end{align}


        \bigskip

        \textbf{Part 2 ($\log_b f(n) \leq d_2 \cdot \log_b g(n)$):}

        \bigskip

        The assumption tells us

        \begin{align}
            f(n) &\leq c_2 \cdot g(n)
        \end{align}

        \bigskip

        Then, it follows from the fact $\forall x,y \in \mathbb{R}^{+}, x \geq y
        \Leftrightarrow \log x \geq \log y$

        \begin{align}
            \log (f(n)) &\leq \log (c_2 \cdot g(n))
        \end{align}

        \bigskip

        Then, using the fact $b > 1$, we can calculate

        \begin{align}
            \frac{\log (f(n))}{\log b} &\leq \frac{\log (c_2 \cdot g(n))}{\log b}\\
            \frac{\log (f(n))}{\log b} &\leq \frac{\log (c_2) + \log (g(n))}{\log b}
        \end{align}

        \bigskip

        Then, since $c_2 = b$,

        \begin{align}
            \frac{\log (f(n))}{\log b} &\leq \frac{\log (b) + \log (g(n))}{\log b}
        \end{align}

        \bigskip

        Then, using the fact $g(n)$ is eventually $\geq b$, we can write

        \begin{align}
            \frac{\log (f(n))}{\log b} &\leq \frac{\log (g(n)) + \log (g(n))}{\log b}\\
            \frac{\log (f(n))}{\log b} &\leq \frac{2 \cdot \log (g(n))}{\log b}
        \end{align}

        \bigskip

        Then, since $\frac{\log f(x)}{\log b} = \log_b f(x)$,

        \begin{align}
            \log_b (f(n)) &\leq 2 \cdot \log_b (g(n))
        \end{align}

        \bigskip

        Then, because we know $d_2 = 2$, we can conclude

        \begin{align}
            \log_b (f(n)) &\leq d_2 \cdot \log_b (g(n))
        \end{align}
    \end{proof}

    \textbf{Notes:}

    \begin{itemize}
        \item $\forall x,y \in \mathbb{R}^{+}, x \geq y \Leftrightarrow \log x \geq \log y$


        \item $\exists c_1,c_2,n_0 \in \mathbb{R}^{+},\:\forall n \in \mathbb{N},
        n \geq n_0 \Rightarrow c_1 \cdot g(n) \leq f(n) \leq c2 \cdot g(n)$

        \item \textbf{Definition of Eventually:} $\exists n_0 \in \mathbb{N},
        n \geq n_0 \Rightarrow P$, where $P:\mathbb{N} \to \{\text{True},\text{False}\}$
    \end{itemize}

    \item

    \begin{proof}
        Let $k \in \mathbb{N}$.

        \bigskip

        First, we will analyze the cost of loop 2 over iteration of loop 1.

        \bigskip

        The code tells us loop 2 starts at $j_k = 1$ with $j_k$ increasing by
        a factor of 3 per iteration until $j_k \geq 1$.

        \bigskip

        Using these facts, we can calculate that the terminating condition occurs
        when

        \setcounter{equation}{0}
        \begin{align}
            3^k &\geq i\\
            k &\geq \log_3 i
        \end{align}

        \bigskip

        Because we know the number of iterations is the smallest value of $k$
        satisfying the above inequality, we can conclude loop 2 has

        \begin{align}
            \lceil \log_3 i \rceil
        \end{align}

        iterations.

        \bigskip

        Next, we need to determine the total number of iterations of loop 2
        over all iterations of loop 1.

        \bigskip

        The code tells us loop 1 starts at $i = 1$ and ends at $i = n$ with each
        $i$ increasing by 1 per iteration.

        \bigskip

        Using these facts, we can conclude loop 2 has total of

        \begin{align}
            \lceil \log_3 1 \rceil + \lceil \log_3 2 \rceil + \cdots + \lceil \log_3 n \rceil &= \sum\limits_{i=1}^n \lceil \log_3 i \rceil
        \end{align}

        iterations.
    \end{proof}

    \item
    After scratching head and looking at solution many times, I realized that there
    are many things I do not yet understand, and it's the best to write what I have
    and learn from the solution. Here is my best attempt :).

    \begin{proof}
        Let $n \in \mathbb{N}$.

        \bigskip

        The previous answer tells us the exact cost of the algorithm is
        \setcounter{equation}{0}
        \begin{align}
            \sum\limits_{i=1}^n \lceil \log_3 i \rceil
        \end{align}

        Then, it follows by changing the variable $i$ to $i' = \log_3 i$ we can write

        \begin{align}
            \sum\limits_{i'=0}^{\lceil \log_3 n \rceil} i'
        \end{align}

        \bigskip

        Then, because we know $\sum\limits_{i=0}^n i = \frac{n(n+1)}{2}$, we can
        conclude

        \begin{align}
            \sum\limits_{i'=0}^{\lceil \log_3 n \rceil} i' &= \frac{(\lceil \log_3 n \rceil)(\lceil \log_3 n \rceil + 1)}{2}\\
            &= \frac{\lceil \log_3 n \rceil^2 + \lceil \log_3 n\rceil}{2}
        \end{align}

        \bigskip

        Then, we can conclude the runtime of the algorithm is $\Theta(\log_3^2 n)$.
    \end{proof}

    \bigskip

    \begin{mdframed}
        \underline{\textbf{Correct Solution:}}

        \bigskip

        \color{red}
        We need to determne $\Theta$ of the algorithm.

        \bigskip

        We will prove that the $\Theta$ of the algorithm is $\Theta(n\log n)$.

        \bigskip

        The answer to previous question tells us the total exact cost of the
        algorithm is

        \begin{align}
            \sum\limits_{i=1}^n \lceil \log_3 i \rceil
        \end{align}

        \bigskip

        Then, by using fact 1 $\forall x \in \mathbb{R}, x \leq \lceil x \rceil \leq x + 1$,
        we can calculate

        \begin{align}
            \sum\limits_{i=1}^n \log_3 i &\leq \sum\limits_{i=1}^n \lceil \log_3 i \rceil \leq \sum\limits_{i=1}^n \Bigl( \log_3 i + 1 \Bigr)\\
            \sum\limits_{i=1}^n \log_3 i &\leq \sum\limits_{i=1}^n \lceil \log_3 i \rceil \leq \Bigl(\sum\limits_{i=1}^n \log_3 i + \sum\limits_{i=1}^n 1 \Bigr)\\
            \sum\limits_{i=1}^n \log_3 i &\leq \sum\limits_{i=1}^n \lceil \log_3 i \rceil \leq \sum\limits_{i=1}^n \log_3 i + n
        \end{align}

        \bigskip

        Then,

        \begin{align}
            \log_3 \Bigl(\prod\limits_{i=1}^n i \Bigr) &\leq \sum\limits_{i=1}^n \lceil \log_3 i \rceil \leq \log_3 \Bigl(\prod\limits_{i=1}^n i \Bigr) + n\\
            \log_3 (n!) &\leq \sum\limits_{i=1}^n \lceil \log_3 i \rceil \leq \log_3 (n!) + n
        \end{align}

        by the fact $\forall a,b \in \mathbb{R}^{+}, \log (a) + \log (b) = \log (ab)$.

        \bigskip

        Then,

        \begin{align}
            \frac{\ln n!}{\ln 3} &\leq \sum\limits_{i=1}^n \lceil \log_3 i \rceil \leq \frac{\ln (n!)}{\ln 3} + n
        \end{align}

        by changing the base to $e$ using the formula $\log_3 n! = \frac{\log_e n!}{\log_e 3} = \frac{\ln n!}{\ln 3}$.

        \bigskip

        Now, the fact 2 tells us $n! \in \Theta (e^{n\ln n - n + \frac{1}{2}\ln n})$.

        \bigskip

        Because we know from fact 3 that $n\ln n - n + \frac{1}{2}\ln n$ is
        eventually $\geq 1$, we can conclude $e^{n\ln n - n \frac{1}{2} \ln n}$ is
        eventually $\geq e$.

        \bigskip

        Since $n!$ is also eventually $\geq e$, by using solution to problem 1.a with
        $g(n) = n!$ and $f(n) = e^{n\ln n - n + \frac{1}{2}\ln }$ and $b = e$,
        we can write

        \begin{align}
            \ln(n!) \in \Theta(\ln(e^{n\ln n - n + \frac{1}{2}\ln n}))\\
            \ln(n!) \in \Theta(n\ln n - n + \frac{1}{2}\ln n)
        \end{align}

        \bigskip

        Then,

        \begin{align}
            \ln(n!) \in \Theta(n\ln n)
        \end{align}

        by the fact $n \ln n - n + \frac{1}{2} \ln n \in \Theta(n \ln n)$.

        \bigskip

        So, since the algorithm runs at least $\frac{\ln n!}{\ln 3}$, we can
        conclude it has asymptotic lower bound of $\Omega(n \ln n)$, and since
        the algorithm runs at most $\frac{\ln n!}{\ln 3} + n$, we can conclude it
        has upper bound running time of $\mathcal{O}(n\ln n)$.

        \bigskip

        Since the value of $\Omega$ and $\mathcal{O}$ are the same, we can conclude
        the algorithm has running time of $\Theta(n \ln n)$ or $\Theta(n \log n)$.
        \color{black}

    \end{mdframed}

    \bigskip

    \textbf{Notes:}

    \begin{itemize}
        \item In a main flow of proof, when there is a huge interruption like
        showing $\ln(n!) \in \Theta(n\ln n)$, how can a sentence be started to tell
        the audience we are working on another major idea?

        \item When an interruption in proof has been occured for another
        major part of a proof, how can a sentence be started to combine
        parts together?

        \item How can a sentence be written to say condition $x_1$, $x_2$, and $x_3$
        are satisfied, so a statement $y$ can be used to an equation or an idea?

    \end{itemize}
\end{enumerate}

\section*{Question 2}
\begin{enumerate}[a.]
    \item

    We need to evaluate tight asymptotic upper bound.

    \bigskip

    We will prove that the tight asymptotic upper bound of the algorithm is
    $\mathcal{O}(n^2)$.

    \bigskip

    First, we need to analyze the number of iterations of loop 2 per iteration of
    loop 1.

    \bigskip

    The code tells us loop 2 starts at $j = 0$ and ends at most$j = i - 1$ with
    $j$ increasing by 1 per iteration.

    \bigskip

    Then, using these facts, we can conclude loop 2 has at most

    \setcounter{equation}{0}
    \begin{align}
        \left\lceil \frac{i-1-0+1}{1} \right\rceil = i
    \end{align}

    iterations.

    \bigskip

    Next, we need to determine the total number of iterations of loop 2 over all
    iterations of loop 1.

    \bigskip

    The code tells us that loop 1 starts at $i = n$ and ends at most $i = 0$ with
    $i$ decreasing by 1 per iteration.

    \bigskip

    Because we know each iteration of loop 1 takes $i$ iterations by loop 2, using
    these facts, we can conclude the total number of iterations of loop 2 is at most

    \begin{align}
        n + (n-1) + (n-2) + \cdots + 0 &= \sum\limits_{i=1}^n\\
        &= \frac{n(n+1)}{2}
    \end{align}

    iterations, or $\mathcal{O}(n^2)$.

    \bigskip

    \begin{mdframed}
        \underline{\textbf{Correct Solution:}}

        \bigskip

        We need to evaluate tight asymptotic upper bound.

        \bigskip

        We will prove that the tight asymptotic upper bound of the algorithm is
        $\mathcal{O}(n^2)$.

        \bigskip

        \color{red}First, we need to analyze the cost of loop 2.\color{black}

        \bigskip

        The code tells us loop 2 starts at $j = 0$ and ends at most$j = i - 1$ with
        $j$ increasing by 1 per iteration.

        \bigskip

        \color{red}
        Then, since each iteration of loop 2 takes a constant step (1 step), using these facts,
        we can conclude the cost of loop 2 is at most

        \setcounter{equation}{0}
        \begin{align}
            1 \cdot (i-1-0+ 1) = i
        \end{align}

        steps.
        \color{black}

        \bigskip

        \color{red}Next, we need to determine cost of loop 1.\color{black}

        \bigskip

        The code tells us that loop 1 starts at $i = n$ and ends at most $i = 0$ with
        $i$ decreasing by 1 per iteration.

        \bigskip

        Because we know each iteration of loop 1 takes \color{red} $i+1$ steps (where $i$
        is from loop 2 and $1$ from line 8)\color{black}, using these facts, we can
        conclude the total cost of \color{red}loop 1 is at most

        \begin{align}
            (n+1) + n + (n-1) + (n-2) + \cdots + 1 &= \sum\limits_{i=0}^n (i + 1)\\
            &= \sum\limits_{i=0}^n i + \sum\limits_{i=0}^n 1\\
            &= \sum\limits_{i=0}^n i + (n+1)\\
            &= \frac{n(n+1)}{2} + (n+1)\\
            &= \frac{(n+1)(n+2)}{2}
        \end{align}

        steps.

        \bigskip

        Finally, adding the cost of line 6, we can conclude the algorithm has total
        cost of $\frac{(n+1)(n+2)}{2} + 1$ steps, which is $\mathcal{O}(n^2)$.
        \color{black}
    \end{mdframed}

    \bigskip

    \textbf{Notes:}

    \begin{itemize}
        \item Noticed professor writes proof that gets to a point (i.e. ... where
        each iteration takes \textbf{$i + 1$ steps}), and provides more detailed explanation
        in brackets (i.e. ... where each iteration takes $i + 1$ steps \textbf{(Adding
        the cost of loop 2 and 1 step for other constant time operations)}).

        \item Noticed professor uses 'finally' when proof has reached the final
        step that leads to its conclusion.
    \end{itemize}

    \item

    Let $n,k \in \mathbb{N}$, and $list = [0,0,\dots,0,1,0,\dots,0]$ where 1 is
    at $\lceil \frac{n}{2} \rceil$ position.

    \bigskip

    We will prove that the tight asymptotic lower bound running time of this
    algorithm is $\Omega(n^2)$.

    \bigskip

    First, we need to evaluate the cost of loop 2.

    \bigskip

    The code tells us loop 2 starts at $j_k = 0$, and $j_k$ will increase by 1 until
    $j_k \geq \lceil \frac{n}{2} \rceil + 1$ (where +1 is because of loop 2
    stopping at $j_k = \lceil \frac{n}{2} \rceil$ by the if condition on line 10).

    \bigskip

    Using the fact $j_k = k+1$, we can calculate that loop 2 stops when
    \setcounter{equation}{0}
    \begin{align}
        k+1 &\geq \left\lceil \frac{n}{2} \right\rceil + 1\\
        k &\geq \left\lceil \frac{n}{2} \right\rceil
    \end{align}

    \bigskip

    Since we are looking for the smallest value of $k$ (because the smallest value of
    k translates to number of iterations), we can conclude the
    loop has

    \begin{align}
        \left\lceil \frac{n}{2} \right\rceil
    \end{align}

    iterations.

    \bigskip

    Since each iteration takes a constant time (1 step), the cost of loop 2
    is

    \begin{align}
        \left\lceil \frac{n}{2} \right\rceil \cdot 1 = \left\lceil \frac{n}{2} \right\rceil
    \end{align}

    steps.

    \bigskip

    Next, we need to evaluate the cost of loop 1.

    \bigskip

    The code tell us loop 1 will start at $i_k = n$, and $i_k$ will decrease by 1
    per iteration until $i_k \leq \lceil \frac{n}{2} \rceil$.

    \bigskip

    Using the fact $i_k = k - 1$, we can write loop 1 stops when

    \begin{align}
        k - 1 &\leq \left\lceil \frac{n}{2} \right\rceil\\
        k &\leq \left\lceil \frac{n}{2} \right\rceil + 1
    \end{align}

    \bigskip

    Since we are looking for the largest value of $k$ (because the largest value of
    k translates to number of iterations), we can conclude loop 1
    has

    \begin{align}
        \left\lceil \frac{n}{2} \right\rceil + 1
    \end{align}

    iterations.

    \bigskip

    Since each costs $\left\lceil \frac{n}{2} \right\rceil + 1$ steps, we can
    conclude loop 1 has cost of

    \begin{align}
        \left(\left\lceil \frac{n}{2} \right\rceil + 1 \right)\left(\left\lceil \frac{n}{2} \right\rceil + 1 \right) &= \left\lceil \frac{n}{2} \right\rceil^2 + 2 \cdot \left\lceil \frac{n}{2} \right\rceil + 1
    \end{align}

    steps.

    \bigskip

    Finally, by adding the cost of line 6 (1 step), the total running time of this algorithm is

    \begin{align}
        \left\lceil \frac{n}{2} \right\rceil^2 + 2 \cdot \left\lceil \frac{n}{2} \right\rceil + 2
    \end{align}

    steps, which is $\Omega(n^2)$

    \bigskip

    \begin{mdframed}
        \underline{\textbf{Correct Solution:}}

        \bigskip

        Let $n,k \in \mathbb{N}$, and $list = [0,0,\dots,0,1,0,\dots,0]$ where 1 is
        at $\lceil \frac{n}{2} \rceil$ position.

        \bigskip

        We will prove that the tight asymptotic lower bound running time of this
        algorithm is $\Omega(n^2)$.

        \bigskip

        First, we need to evaluate the cost of loop 2.

        \bigskip

        The code tells us loop 2 starts at $j_k = 0$, and $j_k$ will increase by 1 until
        $j_k \geq \lceil \frac{n}{2} \rceil + 1$ (where +1 is because of loop 2
        stopping at $j_k = \lceil \frac{n}{2} \rceil$ by the if condition on line 10).

        \bigskip

        Using the fact \color{red}$j_k = k$\color{black}, we can calculate that
        loop 2 stops when

        \setcounter{equation}{0}
        \begin{align}
            k &\geq \left\lceil \frac{n}{2} \right\rceil + 1
        \end{align}

        \bigskip

        Since we are looking for the smallest value of $k$ (because the smallest value of
        k translates to number of iterations), we can conclude the
        loop has

        \color{red}
        \begin{align}
            \left\lceil \frac{n}{2} \right\rceil + 1
        \end{align}
        \color{black}

        iterations.

        \bigskip

        Since each iteration takes a constant time (1 step), the cost of loop 2
        is

        \color{red}
        \begin{align}
            \left( \left\lceil \frac{n}{2} \right\rceil + 1 \right) \cdot 1 = \left\lceil \frac{n}{2} \right\rceil + 1
        \end{align}
        \color{black}

        steps.

        \bigskip

        Next, we need to evaluate the cost of loop 1.

        \bigskip

        The code tell us loop 1 will start at $i_k = n$, and $i_k$ will decrease by 1
        per iteration until $i_k \leq \lceil \frac{n}{2} \rceil$.

        \bigskip

        Using the fact \color{red}$i_k = n - k$\color{black}, we can write loop
        1 stops when

        \begin{align}
            n - k &\leq \left\lceil \frac{n}{2} \right\rceil\\
            -k &\leq \left\lceil \frac{n}{2} \right\rceil - n\\
            k &\geq n - \left\lceil \frac{n}{2} \right\rceil
        \end{align}

        \bigskip

        Since we are looking for the largest value of $k$ (because the largest value of
        k translates to number of iterations), we can conclude loop 1
        has

        \begin{align}
            n - \left\lceil \frac{n}{2} \right\rceil
        \end{align}

        iterations.

        \bigskip

        Since \color{red}each iteration costs $\left\lceil \frac{n}{2} \right\rceil + 2$ steps (where
        $\left\lceil \frac{n}{2} \right\rceil + 1$ is the cost of loop 2 and $+1$
        is the cost of line 14)\color{black}, we can conclude loop 1 has cost of

        \color{red}
        \begin{align}
            \left(\left\lceil \frac{n}{2} \right\rceil + 2 \right)\left( n - \left\lceil \frac{n}{2} \right\rceil \right)
        \end{align}
        \color{black}

        steps.

        \bigskip

        \color{red}Finally, since the loop takes $\left\lceil \frac{n}{2} \right\rceil + 1$
        extra steps (where $\left\lceil \frac{n}{2} \right\rceil$ is the cost of traveling
        from $j = 0$ until $j = \left\lceil \frac{n}{2} \right\rceil$ and $+1$ is the
        cost of line 14) before coming to a full stop, the total running time
        is at least

        \begin{align}
            \left(\left\lceil \frac{n}{2} \right\rceil + 2 \right)\left( n - \left\lceil \frac{n}{2} \right\rceil \right) + \left\lceil \frac{n}{2} \right\rceil + 1
        \end{align}

        steps, which is $\Omega(n^2)$

    \end{mdframed}

    \bigskip

    \textbf{Notes:}

    \begin{itemize}
        \item Noticed there is no room for errors. (most of mark deductions are
        from not being careful with the analysis).
        \item Realized I need to take time to verify and re-verify steps using
        examples at a very fine level (i.e at this step this happens ... at this
        step this happens) until conclusion.
        \item Noticed professor uses $i_k = n - k$ when going backward starting from $n$. And
        for the inequality, $i_k \leq$ is used as opposed to the normal $i_k \geq$.
    \end{itemize}

    \item

    \begin{proof}
        Let $k,n \in \mathbb{N}$.

        \bigskip

        We will prove the statement using proof by cases.

        \bigskip

        \underline{\textbf{Case 1: When all elements in $nums$ are even}}

        Let $nums = [a_1,a_2,\dots,a_n]$ where $a_1,\dots,a_n$ are even numbers.

        \bigskip

        We want to prove the best-case lower bound running time of this algorithm is $\Omega(n)$.

        \bigskip

        First, we need to analyze the cost of loop 2.

        \bigskip

        Given the iteration count $k$, the code tells us, the loop starts at $j_k = 0$
        and increases by 1 per iteration, and so we know $j_k = k$.

        \bigskip

        Because we know loop 2 runs until $j_k \geq i$, we can conclude loop 2 stops when

        \setcounter{equation}{0}
        \begin{align}
            k \geq i
        \end{align}

        \bigskip

        Since we are looking for the smallest value of $k$ (because it represents the
        number of iterations), we can conclude loop 2 has $i$ iterations.

        \bigskip

        Because we know each iteration of loop 2 costs a constant time (1 step), we
        can conclude loop 2 has cost of at least

        \begin{align}
            k \cdot 1 = k
        \end{align}

        steps.

        \bigskip

        Now, we need to evaluate the cost of loop 1.

        \bigskip

        The code tells us loop 1 starts at $i = n$ and ends at $i = n$ due to the truthy
        condition of line 14.

        \bigskip

        Using these facts, we can conclude loop 1 has

        \begin{align}
            \lceil n -n + 1 \rceil = 1
        \end{align}

        iteration.

        \bigskip

        Because we know each iteration of loop 1 costs $i + 2$ steps (where $i$ is
        from the cost of loop 2, and $+2$ are from the cost of line 8 and line 16),
        we can conclude loop 1 has cost of at least

        \begin{align}
            (i + 2) \cdot 1 = i + 2
        \end{align}

        steps.

        \bigskip

        Finally, because we know $i = n$, the total running time is at least $n + 2$,
        which is $\Omega(n)$.

        \bigskip

        \underline{\textbf{Case 2: When one or more elements in $nums$ are odd}}

        \bigskip

        Let $nums = [1,a_2,a_3,\dots,a_{n-1}]$ where $a_2,a_3,\dots,a_{n-1}$ are even numbers.

        \bigskip

        We will prove the algorithm has best-case lower bound running time of $\Omega(n)$.

        \bigskip

        First, we need to evaluate the cost of loop 2.

        \bigskip

        The code tellus us loop 2 starts at $j = 0$ and ends at $j = 0$ due to the
        truthy condition of line 10.

        \bigskip

        Using these facts, we can calculate loop 2 has 1 iteration.

        \bigskip

        Because we know loop 2 takes constant time (1 step) per iteration, we can
        conclude loop 2 has cost of 1 step.

        \bigskip

        Next, we need to evaluate the cost of loop 1.

        \bigskip

        The code tells us that loop 1 starts at $i = n$, and $i$ increases by 1 until
        $i_k \leq -1$, where $k$ represents the iteration count of loop 1.

        \bigskip

        Because we know $i_k = n - k$, we can conclude the loop stops when

        \begin{align}
            n - k &\leq - 1\\
            k &\geq n +  1
        \end{align}

        \bigskip

        Since we are looking for the smallest value of $k$ (because it represents the
        number of iterations), we can conclude loop 1 has

        \begin{align}
            n+1
        \end{align}

        Since each iteration of loop 1 takes 2 steps (where 1 is the cost of loop 2 and
        the other 1 is the cost of line 8), we can conclude that loop 1 has cost of at least

        \begin{align}
            2 \cdot (n+1)
        \end{align}

        steps.

        \bigskip

        Finally, adding the cost of line 8, we can conclude the algorithm has running
        time of at least $2(n+1) + 1$ steps, which is $\Omega(n)$.
    \end{proof}

    \bigskip

    \begin{mdframed}
        \underline{\textbf{Attempt 2:}}

        \bigskip

        \color{red}
        Let $k,n \in \mathbb{N}$.

        \bigskip

        We will prove this statement using proof by cases.
        \color{black}

        \bigskip

        \underline{\textbf{Case 1: When all elements in $nums$ are even}}

        Let $nums = [a_1,a_2,\dots,a_n]$ where $a_1,\dots,a_n$ are even numbers.

        \bigskip

        We want to prove the best-case lower bound running time of this algorithm is $\Omega(n)$.

        \bigskip

        First, we need to analyze the cost of loop 2.

        \bigskip

        Given the iteration count $k$, the code tells us, the loop starts at $j_k = 0$
        and increases by 1 per iteration, and so we know $j_k = k$.

        \bigskip

        Because we know loop 2 runs until $j_k \geq i$, we can conclude loop 2 stops when

        \setcounter{equation}{0}
        \begin{align}
            k \geq i
        \end{align}

        \bigskip

        Since we are looking for the smallest value of $k$ (because it represents the
        number of iterations), we can conclude loop 2 has $i$ iterations.

        \bigskip

        Because we know each iteration of loop 2 costs a constant time (1 step), we
        can conclude loop 2 has cost of at least

        \begin{align}
            k \cdot 1 = k
        \end{align}

        steps.

        \bigskip

        Now, we need to evaluate the cost of loop 1.

        \bigskip

        The code tells us loop 1 starts at $i = n$ and ends at $i = n$ due to the truthy
        condition of line 14.

        \bigskip

        Using these facts, we can conclude loop 1 has

        \begin{align}
            \lceil n -n + 1 \rceil = 1
        \end{align}

        iteration.

        \bigskip

        Because we know each iteration of loop 1 costs $i + 2$ steps (where $i$ is
        from the cost of loop 2, and $+2$ are from the cost of line 8 and line 16),
        we can conclude loop 1 has cost of at least

        \begin{align}
            (i + 2) \cdot 1 = i + 2
        \end{align}

        steps.

        \bigskip

        Finally, because we know $i = n$, the total running time is at least $n + 2$,
        which is $\Omega(n)$.

        \bigskip

        \underline{\textbf{Case 2: When one or more elements in $nums$ are odd}}

        \bigskip

        \color{red}
        In this case, let $m$ be the index of first odd number in $nums$.

        \bigskip

        We need to prove this algorithm has best-case lower bound running time
        of $\Omega(n)$.

        \bigskip

        First, we need to evaluate the cost of loop 2.

        \bigskip

        Given loop 2 iteration count $k$, the code tells us loop 2 starts at $j = 0$,
        and $j$ increases by 1 until $j_k \geq m + 1$.

        \bigskip

        Since we know $j_k = k$, using these facts, we can calculate loop 2 terminates
        when

        \bigskip

        \begin{align}
            k &\geq m + 1
        \end{align}

        \bigskip

        Since we are looking for the smallest value of $k$ (because it represents
        the number of iterations), we can conclude loop 2 has

        \begin{align}
            m + 1
        \end{align}

        iterations.

        \bigskip

        Next, we need to evaluate the cost of loop 1.

        \bigskip

        Given loop 1 iteration count $k$,, The code tells us that loop 1 starts at
        $i = n$, and $i$ decreases by 1 until $i_k \leq m-1$.

        \bigskip

        Since we know $i_k = n - k$, using these facts, we can calculate loop 1 stops
        when

        \begin{align}
            n - k &\leq m - 1\\
            k &\geq n - m +  1
        \end{align}

        \bigskip

        Since we are looking for the smallest value of $k$ (because it represents the
        number of iterations), we can conclude loop 1 has

        \begin{align}
            n-m+1
        \end{align}

        iterations.

        \bigskip

        Because we know that for the first $n-k$ iterations, each iteration of loop 1
        costs $m + 2$ steps (where $m+1$ is the cost of loop 2 and $+1$ is the cost
        of line 8), and last iteration of loop 1 costs another $m + 2$ (where $m$
        is the cost of loop 2 and $+2$ are the cost of line 8 and 15), we can conclude
        loop 1 has cost of

        \begin{align}
            (n-m+1)(m+2)
        \end{align}

        steps.

        \bigskip

        Next, adding the cost of line 6, we can conclude the algorithm has total
        cost of at least

        \begin{align}
            (n-m+1)(m+2) + 1
        \end{align}

        steps.

        \bigskip

        Finally, we need to show this algorithm has runtime of $\Omega(n)$.

        \bigskip

        Using the total cost of algorithm, we can calculate

        \begin{align}
            (n-m+1)(m+2) + 1 &= (n-m)(m+2) + (m+2) + 1\\
            &> (n-m)(m+2) + (m+2)\\
            &= (n-m)m + 2(n-m) + (m+2)\\
            &> (n-m)m + (n-m) + m\\
            &= (n-m)m + n
        \end{align}

        \bigskip

        Because we know $n - m \geq 0$ and $m \geq 0$, we can conclude that

        \begin{align}
            (n-m+1)(m+2) + 1 &> n
        \end{align}

        and the algorithm has best case lower bound running time of $\Omega(n)$.
        \color{black}
    \end{mdframed}

    \bigskip

    \textbf{Notes:}

    \begin{itemize}
        \item The solution in problem 2.b adds constant time operations
        into total cost where as the solution to this problem doesn't... Is there
        a rule behind when and when not they can be included?

        \item Noticed professor reduces the exact cost to $n$ by separating it
        from the rest of the terms

        \begin{mdframed}
            \begin{align*}
                (n-m+1)(m+2) > (n-m)m + n
            \end{align*}
        \end{mdframed}

        \item Realized the best-case lower bound running time doesn't use input family
        like worst-case lower bound running time
    \end{itemize}

\end{enumerate}

\section*{Question 3}
\begin{enumerate}[a.]
    \item

    \begin{proof}
        Let $n \in \mathbb{N}$ and $lst$ be a list with all negative numbers.

        \bigskip

        Then, the code tells us line 9-12 will run for all elements in the list.

        \bigskip

        Because we know $i$ increases by a factor of 2 per iteration, we can conclude
        that at $k^{th}$ iteration, $i$ has value of $i_k = 2^k$.

        \bigskip

        Because we know loop terminates when $i_k \geq n$, we can conclude this is true
        when

        \setcounter{equation}{0}
        \begin{align}
            2^k &\geq n\\
            k &\geq \log n
        \end{align}

        Since we are looking for the smallest value of $k$ (since it represents the
        number of iterations), we can conclude loop has

        \begin{align}
            \lceil \log n \rceil
        \end{align}

        iterations.

        \bigskip

        Since each iteration of while loop takes a constant time (1 step), we can
        conclude the loop has cost of

        \begin{align}
            \lceil \log n \rceil
        \end{align}

        steps.

        \bigskip

        Finally, since lines 2 to 4 have cost of 1 each, by adding to the costs together,
        we can conclude the algorithm has total running time of $\lceil \log n \rceil + 3$,
        which is $\Theta(\log n)$.
    \end{proof}

    \bigskip

    \begin{mdframed}
        \underline{\textbf{Correct Solution:}}

        \bigskip

        Let $n,k \in \mathbb{N}$ and $lst$ be a list with all negative numbers.

        \bigskip

        \color{red}
        In this case, the loop follows this pattern

        \begin{itemize}
            \item \textbf{iteration 1} - else condition executes and $j$ increases by a
            factor of 2
            \item \textbf{iteration 2} - if condition executes and $i$ increases by a
            factor of 2, and moves to where $j$ is
            \item \textbf{iteration 3} - else condition executes again and $j$ increases
            by a factor of 2
            \item \textbf{iteration 4} - if condition executes again and $i$ increases
            by a factor of 2 and moves to where $j$ is.
            \item and this pattern repeats until the end of while loop.

        \end{itemize}

        \bigskip

        Now, we need to determine the total number of iterations in while loop.

        \color{black}

        \bigskip

        Because we know $i$ increases by a factor of 2 per \color{red}execution of
        \textbf{if lst[i] $>=$ 0:} condition, we can conclude that at $k^{th}$ \color{red}execution
        of \textbf{if lst[i] $>=$ 0:} condition\color{black}, $i$ has value of $i_k = 2^k$.

        \bigskip

        Because we know loop terminates when $i_k \geq n$, we can conclude this is true
        when

        \setcounter{equation}{0}
        \begin{align}
            2^k &\geq n\\
            k &\geq \log n
        \end{align}

        Since we are looking for the smallest value of $k$ (since it represents the
        number of \color{red}executions caused by the \textbf{if lst[i] $>=$ 0:} condition\color{black}),
        we can conclude loop has

        \begin{align}
            \lceil \log n \rceil
        \end{align}

        \color{red}executions due to the \textbf{if lst[i] $>=$ 0:} condition\color{black}.

        \bigskip

        \color{red}
        Because we know every execution of \textbf{if lst[i] $>=$ 0:} condition in an iteration,
        is followed by the execution of \textbf{else:} condition in previous iteration,
        we can conclude while loop has total of

        \begin{align}
            2 \cdot \lceil \log n \rceil
        \end{align}

        executions, or iterations.
        \color{black}

        \bigskip

        Since each iteration of while loop takes a constant time (1 step), we can
        conclude the while loop has cost of

        \color{red}
        \begin{align}
            2 \cdot \lceil \log n \rceil
        \end{align}
        \color{black}

        steps.

        \bigskip

        Finally, \color{red}adding cost of 1 for the constant time operations on line 2-4\color{black},
        we can conclude the algorithm has total running time of
        \color{red}$2 \cdot \lceil \log n \rceil + 1$ steps\color{black},
        which is $\Theta(\log n)$.

    \end{mdframed}

    \bigskip

    \textbf{Notes:}

    \begin{itemize}
        \item Noticed professor bundles up time of constant operations (i.e. line 2-4)
        to 1, and same for the ones within while loop.
        \item Noticed professor introduces $k$ in body as '$k^{th}$ execution of
        the if/else branch', and he doesn't introduce the variable in header.
        \item Noticed professor uses the word 'execution' to focus on the number
        of iterations caused by the if condition.
        \item Noticed professor lays out the pattern in while loop before moving
        onto proof.
    \end{itemize}

    \item

    \underline{\textbf{Notes:}}

    \bigskip

    \begin{itemize}
        \item

        I analyzed the example $[0,0,0,0,-1,-1,-1,-1]$. This is what I found.

        \begin{itemize}
            \item \textbf{iteration 1:} if brach of statement executes and $i$ increases
            by a 1 ($i = 1$, $j = 1$)
            \item \textbf{iteration 2:} if brach of statement executes and $i$ increases
            by a 1 ($i = 2$, $j = 1$)
            \item \textbf{iteration 3:} if brach of statement executes and $i$ increases
            by a 1 ($i = 3$, $j = 1$)
            \item \textbf{iteration 4:} if brach of statement executes and $i$ increases
            by a 1 ($i = 4$, $j = 1$)
            \item \textbf{iteration 4:} else branch of statement executes, causing
            \textbf{lst[i] = abs(lst[i])}, $i = 0$, and $j$ to increase by twice of its size ($i = 0$, $j = 2$)

            The following is how the list looks after update

            \begin{align*}
                [0,0,0,0,1,-1,-1,-1]
            \end{align*}

            \item \textbf{iteration 5:} if brach of statement executes and $i$ increases
            by a 2 ($i = 2$, $j = 2$)
            \item \textbf{iteration 5:} if brach of statement executes and $i$ increases
            by a 2 ($i = 4$, $j = 2$)
            \item \textbf{iteration 5:} if brach of statement executes and $i$ increases
            by a 2 ($i = 6$, $j = 2$)

            \item \textbf{iteration 6:} else branch of statement executes, causing
            \textbf{lst[i] = abs(lst[i])}, $i = 0$, and $j$ to increase by twice of its size ($i = 0$, $j = 4$)

            The following is how the list looks after update

            \begin{align*}
                [0,0,0,0,1,-1,1,-1]
            \end{align*}

            \item \textbf{iteration 7:} if brach of statement executes and $i$ increases
            by a 4 ($i = 4$, $j = 4$)

            \item \textbf{iteration 8:} if brach of statement executes and $i$ increases
            by a 4 ($i = 8$, $j = 4$)

            \item \textbf{iteration 9:} Loop terminates
        \end{itemize}

    \end{itemize}

\end{enumerate}

\section*{Question 4}

\end{document}