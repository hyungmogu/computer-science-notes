\documentclass[12pt]{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage{enumerate}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mdframed}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{xcolor}


\begin{document}
\title{Problem Set 4 Solution}
\author{Hyungmo Gu}
\maketitle

\section*{Question 1}
\begin{enumerate}[a.]
    \item

    \textbf{Statement:} $\forall f,g:\mathbb{N} \to \mathbb{R}^{+}$,
    $b \in \mathbb{R}^{+}$, $(g(n) \in \Theta(f(n))) \land (n_0 \in \mathbb{N},\:
    n \geq n_0 \Rightarrow f(n) \geq b \land g(n) \geq b) \land (b > 1) \Rightarrow
    \log_b(g(n)) \in \Theta(\log_b(f(n)))$

    \bigskip

    \textbf{Statement Expanded:} $\forall f,g:\mathbb{N} \to \mathbb{R}^{+}$,
    $b \in \mathbb{R}^{+}$, $\Bigl(\exists c_1,c_2,n_0 \in \mathbb{R}^{+},\:\forall n \in \mathbb{N},\:
    n \geq n_0 \Rightarrow c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n)\Bigr) \land \Bigl(\exists n_1 \in \mathbb{N},\:
    n \geq n_1 \Rightarrow f(n) \geq b \land g(n) \geq b \Bigr) \land \Bigl( b > 1 \Bigr) \Rightarrow
    \Bigl(\exists d_1,d_2,n_2 \in \mathbb{R}^{+},\:\forall n \in \mathbb{N},\: n \geq n_2
    \Rightarrow d_1 \cdot \log_b(g(n)) \leq \log_b(f(n)) \leq d_2 \cdot \log_b(g(n) \Bigr)$

    \bigskip

    \begin{proof}
        Let $f,g:\mathbb{N} \to \mathbb{R}^{+}$, and $b \in \mathbb{R}^{+}$. Assume
        $c_1 = 1$, $c_2 = b$, and $n_0 = 1$, and $n \in \mathbb{N}$ such that
        $n \geq n_0$ and $c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n)$. Assume $f(n)$
        and $g(n)$ are eventually $\geq b$. Assume $b > 1$. Let $d_1 = 1$, $d_2 = 2$,
        and $n_2 = n_0$. Assume $n \geq n_2$.

        \bigskip

        We need to show $d_1 \cdot \log_b g(n) \leq \log_b f(n) \leq d_2 \cdot \log_b g(n)$.

        \bigskip

        We will do so in two parts. One for $(d_1 \cdot \log_b g(n) \leq \log_b f(n))$ and
        the other for $(\log_b f(n) \leq d_2 \cdot \log_b g(n))$.

        \bigskip

        \textbf{Part 1 ($d_1 \cdot \log_b g(n) \leq \log_b f(n)$):}

        \bigskip

        The assumption tell us

        \begin{align}
            c_1 \cdot g(n) \leq f(n)
        \end{align}

        \bigskip

        Then, it follows from the fact $\forall x,y \in \mathbb{R}^{+}, x \geq y
        \Leftrightarrow \log x \geq \log y$

        \begin{align}
            \log (c_1 \cdot g(n)) &\leq \log (f(n))
        \end{align}

        \bigskip

        Then, using the fact $b > 1$, we can calculate

        \begin{align}
            \frac{\log (c_1 \cdot g(n))}{\log b} &\leq \frac{\log (f(n))}{\log b}\\
            \frac{\log (c_1) + \log (g(n))}{\log b} &\leq \frac{\log (f(n))}{\log b}
        \end{align}

        \bigskip

        Then,

        \begin{align}
            \frac{\log (g(n))}{\log b} &\leq \frac{\log (f(n))}{\log b}
        \end{align}

        by the fact $c_1 = 1$ and $\log c_1 = 0$.

        \bigskip

        Then, since $\frac{\log f(x)}{\log b} = \log_b f(x)$,

        \begin{align}
            \log_b (g(n)) &\leq \log_b (f(n))
        \end{align}

        \bigskip

        Then, because we know $d_1 = 1$, we can conclude

        \begin{align}
            \log_b (g(n)) &\leq d_1 \cdot \log_b (f(n))
        \end{align}


        \bigskip

        \textbf{Part 2 ($\log_b f(n) \leq d_2 \cdot \log_b g(n)$):}

        \bigskip

        The assumption tells us

        \begin{align}
            f(n) &\leq c_2 \cdot g(n)
        \end{align}

        \bigskip

        Then, it follows from the fact $\forall x,y \in \mathbb{R}^{+}, x \geq y
        \Leftrightarrow \log x \geq \log y$

        \begin{align}
            \log (f(n)) &\leq \log (c_2 \cdot g(n))
        \end{align}

        \bigskip

        Then, using the fact $b > 1$, we can calculate

        \begin{align}
            \frac{\log (f(n))}{\log b} &\leq \frac{\log (c_2 \cdot g(n))}{\log b}\\
            \frac{\log (f(n))}{\log b} &\leq \frac{\log (c_2) + \log (g(n))}{\log b}
        \end{align}

        \bigskip

        Then, since $c_2 = b$,

        \begin{align}
            \frac{\log (f(n))}{\log b} &\leq \frac{\log (b) + \log (g(n))}{\log b}
        \end{align}

        \bigskip

        Then, using the fact $g(n)$ is eventually $\geq b$, we can write

        \begin{align}
            \frac{\log (f(n))}{\log b} &\leq \frac{\log (g(n)) + \log (g(n))}{\log b}\\
            \frac{\log (f(n))}{\log b} &\leq \frac{2 \cdot \log (g(n))}{\log b}
        \end{align}

        \bigskip

        Then, since $\frac{\log f(x)}{\log b} = \log_b f(x)$,

        \begin{align}
            \log_b (f(n)) &\leq 2 \cdot \log_b (g(n))
        \end{align}

        \bigskip

        Then, because we know $d_2 = 2$, we can conclude

        \begin{align}
            \log_b (f(n)) &\leq d_2 \cdot \log_b (g(n))
        \end{align}
    \end{proof}

    \textbf{Notes:}

    \begin{itemize}
        \item $\forall x,y \in \mathbb{R}^{+}, x \geq y \Leftrightarrow \log x \geq \log y$


        \item $\exists c_1,c_2,n_0 \in \mathbb{R}^{+},\:\forall n \in \mathbb{N},
        n \geq n_0 \Rightarrow c_1 \cdot g(n) \leq f(n) \leq c2 \cdot g(n)$

        \item \textbf{Definition of Eventually:} $\exists n_0 \in \mathbb{N},
        n \geq n_0 \Rightarrow P$, where $P:\mathbb{N} \to \{\text{True},\text{False}\}$
    \end{itemize}

    \item

    \begin{proof}
        Let $k \in \mathbb{N}$.

        \bigskip

        First, we will analyze the cost of loop 2 over iteration of loop 1.

        \bigskip

        The code tells us loop 2 starts at $j_k = 1$ with $j_k$ increasing by
        a factor of 3 per iteration until $j_k \geq 1$.

        \bigskip

        Using these facts, we can calculate that the terminating condition occurs
        when

        \setcounter{equation}{0}
        \begin{align}
            3^k &\geq i\\
            k &\geq \log_3 i
        \end{align}

        \bigskip

        Because we know the number of iterations is the smallest value of $k$
        satisfying the above inequality, we can conclude loop 2 has

        \begin{align}
            \lceil \log_3 i \rceil
        \end{align}

        iterations.

        \bigskip

        Next, we need to determine the total number of iterations of loop 2
        over all iterations of loop 1.

        \bigskip

        The code tells us loop 1 starts at $i = 1$ and ends at $i = n$ with each
        $i$ increasing by 1 per iteration.

        \bigskip

        Using these facts, we can conclude loop 2 has total of

        \begin{align}
            \lceil \log_3 1 \rceil + \lceil \log_3 2 \rceil + \cdots + \lceil \log_3 n \rceil &= \sum\limits_{i=1}^n \lceil \log_3 i \rceil
        \end{align}

        iterations.
    \end{proof}

    \item
    After scratching head and looking at solution many times, I realized that there
    are many things I do not yet understand, and it's the best to write what I have
    and learn from the solution. Here is my best attempt :).

    \begin{proof}
        Let $n \in \mathbb{N}$.

        \bigskip

        The previous answer tells us the exact cost of the algorithm is
        \setcounter{equation}{0}
        \begin{align}
            \sum\limits_{i=1}^n \lceil \log_3 i \rceil
        \end{align}

        Then, it follows by changing the variable $i$ to $i' = \log_3 i$ we can write

        \begin{align}
            \sum\limits_{i'=0}^{\lceil \log_3 n \rceil} i'
        \end{align}

        \bigskip

        Then, because we know $\sum\limits_{i=0}^n i = \frac{n(n+1)}{2}$, we can
        conclude

        \begin{align}
            \sum\limits_{i'=0}^{\lceil \log_3 n \rceil} i' &= \frac{(\lceil \log_3 n \rceil)(\lceil \log_3 n \rceil + 1)}{2}\\
            &= \frac{\lceil \log_3 n \rceil^2 + \lceil \log_3 n\rceil}{2}
        \end{align}

        \bigskip

        Then, we can conclude the runtime of the algorithm is $\Theta(\log_3^2 n)$.
    \end{proof}

    \bigskip

    \begin{mdframed}
        \underline{\textbf{Correct Solution:}}

        \bigskip

        \color{red}
        We need to determne $\Theta$ of the algorithm.

        \bigskip

        We will prove that the $\Theta$ of the algorithm is $\Theta(n\log n)$.

        \bigskip

        The answer to previous question tells us the total exact cost of the
        algorithm is

        \begin{align}
            \sum\limits_{i=1}^n \lceil \log_3 i \rceil
        \end{align}

        \bigskip

        Then, by using fact 1 $\forall x \in \mathbb{R}, x \leq \lceil x \rceil \leq x + 1$,
        we can calculate

        \begin{align}
            \sum\limits_{i=1}^n \log_3 i &\leq \sum\limits_{i=1}^n \lceil \log_3 i \rceil \leq \sum\limits_{i=1}^n \Bigl( \log_3 i + 1 \Bigr)\\
            \sum\limits_{i=1}^n \log_3 i &\leq \sum\limits_{i=1}^n \lceil \log_3 i \rceil \leq \Bigl(\sum\limits_{i=1}^n \log_3 i + \sum\limits_{i=1}^n 1 \Bigr)\\
            \sum\limits_{i=1}^n \log_3 i &\leq \sum\limits_{i=1}^n \lceil \log_3 i \rceil \leq \sum\limits_{i=1}^n \log_3 i + n
        \end{align}

        \bigskip

        Then,

        \begin{align}
            \log_3 \Bigl(\prod\limits_{i=1}^n i \Bigr) &\leq \sum\limits_{i=1}^n \lceil \log_3 i \rceil \leq \log_3 \Bigl(\prod\limits_{i=1}^n i \Bigr) + n\\
            \log_3 (n!) &\leq \sum\limits_{i=1}^n \lceil \log_3 i \rceil \leq \log_3 (n!) + n
        \end{align}

        by the fact $\forall a,b \in \mathbb{R}^{+}, \log (a) + \log (b) = \log (ab)$.

        \bigskip

        Then,

        \begin{align}
            \frac{\ln n!}{\ln 3} &\leq \sum\limits_{i=1}^n \lceil \log_3 i \rceil \leq \frac{\ln (n!)}{\ln 3} + n
        \end{align}

        by changing the base to $e$ using the formula $\log_3 n! = \frac{\log_e n!}{\log_e 3} = \frac{\ln n!}{\ln 3}$.

        \bigskip

        Now, the fact 2 tells us $n! \in \Theta (e^{n\ln n - n + \frac{1}{2}\ln n})$.

        \bigskip

        Because we know from fact 3 that $n\ln n - n + \frac{1}{2}\ln n$ is
        eventually $\geq 1$, we can conclude $e^{n\ln n - n \frac{1}{2} \ln n}$ is
        eventually $\geq e$.

        \bigskip

        Since $n!$ is also eventually $\geq e$, by using solution to problem 1.a with
        $g(n) = n!$ and $f(n) = e^{n\ln n - n + \frac{1}{2}\ln }$ and $b = e$,
        we can write

        \begin{align}
            \ln(n!) \in \Theta(\ln(e^{n\ln n - n + \frac{1}{2}\ln n}))\\
            \ln(n!) \in \Theta(n\ln n - n + \frac{1}{2}\ln n)
        \end{align}

        \bigskip

        Then,

        \begin{align}
            \ln(n!) \in \Theta(n\ln n)
        \end{align}

        by the fact $n \ln n - n + \frac{1}{2} \ln n \in \Theta(n \ln n)$.

        \bigskip

        So, since the algorithm runs at least $\frac{\ln n!}{\ln 3}$, we can
        conclude it has asymptotic lower bound of $\Omega(n \ln n)$, and since
        the algorithm runs at most $\frac{\ln n!}{\ln 3} + n$, we can conclude it
        has upper bound running time of $\mathcal{O}(n\ln n)$.

        \bigskip

        Since the value of $\Omega$ and $\mathcal{O}$ are the same, we can conclude
        the algorithm has running time of $\Theta(n \ln n)$ or $\Theta(n \log n)$.
        \color{black}

    \end{mdframed}

    \bigskip

    \textbf{Notes:}

    \begin{itemize}
        \item In a main flow of proof, when there is a huge interruption like
        showing $\ln(n!) \in \Theta(n\ln n)$, how can a sentence be started to tell
        the audience we are working on another major idea?

        \item When an interruption in proof has been occured for another
        major part of a proof, how can a sentence be started to combine
        parts together?

        \item How can a sentence be written to say condition $x_1$, $x_2$, and $x_3$
        are satisfied, so a statement $y$ can be used to an equation or an idea?

    \end{itemize}
\end{enumerate}

\section*{Question 2}
\begin{enumerate}[a.]
    \item

    We need to evaluate tight asymptotic upper bound.

    \bigskip

    We will prove that the tight asymptotic upper bound of the algorithm is
    $\mathcal{O}(n^2)$.

    \bigskip

    First, we need to analyze the number of iterations of loop 2 per iteration of
    loop 1.

    \bigskip

    The code tells us loop 2 starts at $j = 0$ and ends at most$j = i - 1$ with
    $j$ increasing by 1 per iteration.

    \bigskip

    Then, using these facts, we can conclude loop 2 has at most

    \setcounter{equation}{0}
    \begin{align}
        \left\lceil \frac{i-1-0+1}{1} \right\rceil = i
    \end{align}

    iterations.

    \bigskip

    Next, we need to determine the total number of iterations of loop 2 over all
    iterations of loop 1.

    \bigskip

    The code tells us that loop 1 starts at $i = n$ and ends at most $i = 0$ with
    $i$ decreasing by 1 per iteration.

    \bigskip

    Because we know each iteration of loop 1 takes $i$ iterations by loop 2, using
    these facts, we can conclude the total number of iterations of loop 2 is at most

    \begin{align}
        n + (n-1) + (n-2) + \cdots + 0 &= \sum\limits_{i=1}^n\\
        &= \frac{n(n+1)}{2}
    \end{align}

    iterations, or $\mathcal{O}(n^2)$.

    \bigskip

    \begin{mdframed}
        \underline{\textbf{Correct Solution:}}

        \bigskip

        We need to evaluate tight asymptotic upper bound.

        \bigskip

        We will prove that the tight asymptotic upper bound of the algorithm is
        $\mathcal{O}(n^2)$.

        \bigskip

        \color{red}First, we need to analyze the cost of loop 2.\color{black}

        \bigskip

        The code tells us loop 2 starts at $j = 0$ and ends at most$j = i - 1$ with
        $j$ increasing by 1 per iteration.

        \bigskip

        \color{red}
        Then, since each iteration of loop 2 takes a constant step (1 step), using these facts,
        we can conclude the cost of loop 2 is at most

        \setcounter{equation}{0}
        \begin{align}
            1 \cdot (i-1-0+ 1) = i
        \end{align}

        steps.
        \color{black}

        \bigskip

        \color{red}Next, we need to determine cost of loop 1.\color{black}

        \bigskip

        The code tells us that loop 1 starts at $i = n$ and ends at most $i = 0$ with
        $i$ decreasing by 1 per iteration.

        \bigskip

        Because we know each iteration of loop 1 takes \color{red} $i+1$ steps (where $i$
        is from loop 2 and $1$ from line 8)\color{black}, using these facts, we can
        conclude the total cost of \color{red}loop 1 is at most

        \begin{align}
            (n+1) + n + (n-1) + (n-2) + \cdots + 1 &= \sum\limits_{i=0}^n (i + 1)\\
            &= \sum\limits_{i=0}^n i + \sum\limits_{i=0}^n 1\\
            &= \sum\limits_{i=0}^n i + (n+1)\\
            &= \frac{n(n+1)}{2} + (n+1)\\
            &= \frac{(n+1)(n+2)}{2}
        \end{align}

        steps.

        \bigskip

        Finally, adding the cost of line 6, we can conclude the algorithm has total
        cost of $\frac{(n+1)(n+2)}{2} + 1$ steps, which is $\mathcal{O}(n^2)$.
        \color{black}
    \end{mdframed}

    \bigskip

    \textbf{Notes:}

    \begin{itemize}
        \item Noticed professor writes proof that gets to a point (i.e. ... where
        each iteration takes \textbf{$i + 1$ steps}), and provides more detailed explanation
        in brackets (i.e. ... where each iteration takes $i + 1$ steps \textbf{(Adding
        the cost of loop 2 and 1 step for other constant time operations)}).

        \item Noticed professor uses 'finally' when proof has reached the final
        step that leads to its conclusion.
    \end{itemize}

    \item

    Let $n,k \in \mathbb{N}$, and $list = [0,0,\dots,0,1,0,\dots,0]$ where 1 is
    at $\lceil \frac{n}{2} \rceil$ position.

    \bigskip

    We will prove that the tight asymptotic lower bound running time of this
    algorithm is $\Omega(n^2)$.

    \bigskip

    First, we need to evaluate the cost of loop 2.

    \bigskip

    The code tells us loop 2 starts at $j_k = 0$, and $j_k$ will increase by 1 until
    $j_k \geq \lceil \frac{n}{2} \rceil + 1$ (where +1 is because of loop 2
    stopping at $j_k = \lceil \frac{n}{2} \rceil$ by the if condition on line 10).

    \bigskip

    Using the fact $j_k = k+1$, we can calculate that loop 2 stops when
    \setcounter{equation}{0}
    \begin{align}
        k+1 &\geq \left\lceil \frac{n}{2} \right\rceil + 1\\
        k &\geq \left\lceil \frac{n}{2} \right\rceil
    \end{align}

    \bigskip

    Since we are looking for the smallest value of $k$ (because the smallest value of
    k translates to number of iterations), we can conclude the
    loop has

    \begin{align}
        \left\lceil \frac{n}{2} \right\rceil
    \end{align}

    iterations.

    \bigskip

    Since each iteration takes a constant time (1 step), the cost of loop 2
    is

    \begin{align}
        \left\lceil \frac{n}{2} \right\rceil \cdot 1 = \left\lceil \frac{n}{2} \right\rceil
    \end{align}

    steps.

    \bigskip

    Next, we need to evaluate the cost of loop 1.

    \bigskip

    The code tell us loop 1 will start at $i_k = n$, and $i_k$ will decrease by 1
    per iteration until $i_k \leq \lceil \frac{n}{2} \rceil$.

    \bigskip

    Using the fact $i_k = k - 1$, we can write loop 1 stops when

    \begin{align}
        k - 1 &\leq \left\lceil \frac{n}{2} \right\rceil\\
        k &\leq \left\lceil \frac{n}{2} \right\rceil + 1
    \end{align}

    \bigskip

    Since we are looking for the largest value of $k$ (because the largest value of
    k translates to number of iterations), we can conclude loop 1
    has

    \begin{align}
        \left\lceil \frac{n}{2} \right\rceil + 1
    \end{align}

    iterations.

    \bigskip

    Since each costs $\left\lceil \frac{n}{2} \right\rceil + 1$ steps, we can
    conclude loop 1 has cost of

    \begin{align}
        \left(\left\lceil \frac{n}{2} \right\rceil + 1 \right)\left(\left\lceil \frac{n}{2} \right\rceil + 1 \right) &= \left\lceil \frac{n}{2} \right\rceil^2 + 2 \cdot \left\lceil \frac{n}{2} \right\rceil + 1
    \end{align}

    steps.

    \bigskip

    Finally, by adding the cost of line 6 (1 step), the total running time of this algorithm is

    \begin{align}
        \left\lceil \frac{n}{2} \right\rceil^2 + 2 \cdot \left\lceil \frac{n}{2} \right\rceil + 2
    \end{align}

    steps, which is $\Omega(n^2)$

    \bigskip

    \begin{mdframed}
        \underline{\textbf{Correct Solution:}}

        \bigskip

        Let $n,k \in \mathbb{N}$, and $list = [0,0,\dots,0,1,0,\dots,0]$ where 1 is
        at $\lceil \frac{n}{2} \rceil$ position.

        \bigskip

        We will prove that the tight asymptotic lower bound running time of this
        algorithm is $\Omega(n^2)$.

        \bigskip

        First, we need to evaluate the cost of loop 2.

        \bigskip

        The code tells us loop 2 starts at $j_k = 0$, and $j_k$ will increase by 1 until
        $j_k \geq \lceil \frac{n}{2} \rceil + 1$ (where +1 is because of loop 2
        stopping at $j_k = \lceil \frac{n}{2} \rceil$ by the if condition on line 10).

        \bigskip

        Using the fact \color{red}$j_k = k$\color{black}, we can calculate that
        loop 2 stops when

        \setcounter{equation}{0}
        \begin{align}
            k &\geq \left\lceil \frac{n}{2} \right\rceil + 1
        \end{align}

        \bigskip

        Since we are looking for the smallest value of $k$ (because the smallest value of
        k translates to number of iterations), we can conclude the
        loop has

        \color{red}
        \begin{align}
            \left\lceil \frac{n}{2} \right\rceil + 1
        \end{align}
        \color{black}

        iterations.

        \bigskip

        Since each iteration takes a constant time (1 step), the cost of loop 2
        is

        \color{red}
        \begin{align}
            \left( \left\lceil \frac{n}{2} \right\rceil + 1 \right) \cdot 1 = \left\lceil \frac{n}{2} \right\rceil + 1
        \end{align}
        \color{black}

        steps.

        \bigskip

        Next, we need to evaluate the cost of loop 1.

        \bigskip

        The code tell us loop 1 will start at $i_k = n$, and $i_k$ will decrease by 1
        per iteration until $i_k \leq \lceil \frac{n}{2} \rceil$.

        \bigskip

        Using the fact \color{red}$i_k = n - k$\color{black}, we can write loop
        1 stops when

        \begin{align}
            n - k &\leq \left\lceil \frac{n}{2} \right\rceil\\
            -k &\leq \left\lceil \frac{n}{2} \right\rceil - n\\
            k &\geq n - \left\lceil \frac{n}{2} \right\rceil
        \end{align}

        \bigskip

        Since we are looking for the largest value of $k$ (because the largest value of
        k translates to number of iterations), we can conclude loop 1
        has

        \begin{align}
            n - \left\lceil \frac{n}{2} \right\rceil
        \end{align}

        iterations.

        \bigskip

        Since \color{red}each iteration costs $\left\lceil \frac{n}{2} \right\rceil + 2$ steps (where
        $\left\lceil \frac{n}{2} \right\rceil + 1$ is the cost of loop 2 and $+1$
        is the cost of line 14)\color{black}, we can conclude loop 1 has cost of

        \color{red}
        \begin{align}
            \left(\left\lceil \frac{n}{2} \right\rceil + 2 \right)\left( n - \left\lceil \frac{n}{2} \right\rceil \right)
        \end{align}
        \color{black}

        steps.

        \bigskip

        \color{red}Finally, since the loop takes $\left\lceil \frac{n}{2} \right\rceil + 1$
        extra steps (where $\left\lceil \frac{n}{2} \right\rceil$ is the cost of traveling
        from $j = 0$ until $j = \left\lceil \frac{n}{2} \right\rceil$ and $+1$ is the
        cost of line 14) before coming to a full stop, the total running time
        is at least

        \begin{align}
            \left(\left\lceil \frac{n}{2} \right\rceil + 2 \right)\left( n - \left\lceil \frac{n}{2} \right\rceil \right) + \left\lceil \frac{n}{2} \right\rceil + 1
        \end{align}

        steps, which is $\Omega(n^2)$

    \end{mdframed}

\end{enumerate}

\section*{Question 3}

\section*{Question 4}

\end{document}